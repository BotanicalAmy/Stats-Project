title = "Cancellations by Online Bookings",
x = "Booking type (1 = Online)",
y = "Count"
) %>%
gf_refine(
scale_fill_manual(values = c("Canceled" = "#FF6B6B", "Not_Canceled" = "#4ECDC4"))
) %>%
gf_theme(
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
legend.position = "bottom"
)
)
gf_bar(~HasChildren, fill=~Status, data = hotel_bookings) %>%
gf_labs(
title = "Cancellation by HasChildren",
x = "Has Children (0 = No, 1 = Yes)",
y = "Count"
) %>%
gf_refine(
scale_fill_manual(values = c("Canceled" = "#FF6B6B", "Not_Canceled" = "#4ECDC4"))
) %>%
gf_theme(
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
legend.position = "bottom"
)
)
gf_bar(~Parking, fill=~Status, data = hotel_bookings) %>%
gf_labs(
title = "Cancellation by Parking Option",
x = "Parking",
y = "Count"
) %>%
gf_refine(
scale_fill_manual(values = c("Canceled" = "#FF6B6B", "Not_Canceled" = "#4ECDC4"))
) %>%
gf_theme(
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
legend.position = "bottom"
)
)
gf_bar(~HasRequests, fill=~Status, data = hotel_bookings) %>%
gf_labs(
title = "Cancellations by HasRequest",
x = "Had a special request",
y = "Count"
) %>%
gf_refine(
scale_fill_manual(values = c("Canceled" = "#FF6B6B", "Not_Canceled" = "#4ECDC4"))
) %>%
gf_theme(
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
legend.position = "bottom"
)
)
gf_bar(~Online, fill=~Status, data = hotel_bookings) %>%
gf_labs(
title = "Cancellations by Online Bookings",
x = "Booking type (1 = Online)",
y = "Count"
) %>%
gf_refine(
scale_fill_manual(values = c("Canceled" = "#FF6B6B", "Not_Canceled" = "#4ECDC4"))
) %>%
gf_theme(
theme_minimal() +
theme(
plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
legend.position = "bottom"
)
)
# function to generate proportion tables
create_prop_table <- function(data, group_var) {
group_var <- enquo(group_var)
data %>%
group_by(!!group_var, Status) %>%
summarise(count = n(), .groups = "drop") %>%
group_by(!!group_var) %>%
mutate(proportion = count / sum(count))
}
# Now use the function with each variable
room_type_table <- create_prop_table(hotel_bookings, RoomType)
meal_table <- create_prop_table(hotel_bookings, Meal)
market_table <- create_prop_table(hotel_bookings, Market)
parking_table <- create_prop_table(hotel_bookings, Parking)
month_table <- create_prop_table(hotel_bookings, Month)
requests_table <- create_prop_table(hotel_bookings, HasRequests)
children_table <- create_prop_table(hotel_bookings, HasChildren)
online_table <- create_prop_table(hotel_bookings, Online)
# Function to create a nicely formatted kable table with rounded proportions
print_pretty_kable <- function(table_data, title) {
table_data <- table_data %>%
mutate(proportion = round(proportion, 3))
kable(table_data, caption = title) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE) %>%
column_spec(1, bold = TRUE) %>%
row_spec(0, bold = TRUE, background = "#E8E8E8")
}
# Print each table with nice formatting
print_pretty_kable(room_type_table, "Room Type")
print_pretty_kable(meal_table, "Meal Option")
print_pretty_kable(market_table, "Market")
print_pretty_kable(parking_table, "Parking")
print_pretty_kable(month_table, "Month")
print_pretty_kable(requests_table, "Special Requests")
print_pretty_kable(children_table, "Has Children")
print_pretty_kable(online_table, "Online Booking")
modr <- glm(factor(Status) ~ HasRequests, family = binomial(link = "logit"), data = hotel_bookings)
# summary(modr)
plot(modr, which = 1)
#confidence interval for HasRequests
confint.default(modr)
mod1 <- glm(factor(Status) ~ RoomType, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod1)
# plot(mod1, which = 1)
mod2 <- glm(factor(Status) ~ Parking, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod2)
# plot(mod2, which = 1)
mod3 <- glm(factor(Status) ~ Online, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod3)
# plot(mod3, which = 1)
mod4 <- glm(factor(Status) ~ Meal, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod4)
# plot(mod4, which = 1)
# mod5 <- glm(factor(Status) ~ Month, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod5)
#plot(mod5, which = 1)
new_model <- glm(factor(Status) ~ LeadTime + Online + HasRequests + factor(Month) + AvgPrice + StayLength, family = binomial(link = "logit"), data=hotel_bookings)
anova_table <- Anova(new_model, test = "LR")
print(anova_table)
# Get the AIC
AIC(new_model)
new_model <- glm(factor(Status) ~ LeadTime + HasRequests + factor(Month) + AvgPrice + StayLength, family = binomial(link = "logit"), data=hotel_bookings)
anova_table <- Anova(new_model, test = "LR")
print(anova_table)
# Get the AIC
AIC(new_model)
new_model <- glm(factor(Status) ~ LeadTime + factor(Market) HasRequests + factor(Month) + AvgPrice + StayLength, family = binomial(link = "logit"), data=hotel_bookings)
new_model <- glm(factor(Status) ~ LeadTime + factor(Market) + HasRequests + factor(Month) + AvgPrice + StayLength, family = binomial(link = "logit"), data=hotel_bookings)
anova_table <- Anova(new_model, test = "LR")
print(anova_table)
# Get the AIC
AIC(new_model)
modr <- glm(factor(Status) ~ HasRequests, family = binomial(link = "logit"), data = hotel_bookings)
# summary(modr)
plot(modr, which = 1)
#confidence interval for HasRequests
#confint.default(modr)
mod1 <- glm(factor(Status) ~ RoomType, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod1)
# plot(mod1, which = 1)
mod2 <- glm(factor(Status) ~ Parking, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod2)
# plot(mod2, which = 1)
mod3 <- glm(factor(Status) ~ Market, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod3)
# plot(mod3, which = 1)
mod4 <- glm(factor(Status) ~ Meal, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod4)
# plot(mod4, which = 1)
# mod5 <- glm(factor(Status) ~ Month, family = binomial(link = "logit"), data = hotel_bookings)
# summary(mod5)
#plot(mod5, which = 1)
new_model <- glm(factor(Status) ~ LeadTime + factor(Market) + HasRequests + factor(Month) + AvgPrice + StayLength, family = binomial(link = "logit"), data=hotel_bookings)
anova_table <- Anova(new_model, test = "LR")
print(anova_table)
# Other diagnostic plots for the model
plot(new_model, which = c(1, 2))  # Residuals vs Fitted and Normal Q-Q plot
# Get the AIC
AIC(new_model)
new_model <- glm(factor(Status) ~ LeadTime + factor(Market) + HasRequests + factor(Month) + AvgPrice + StayLength, family = binomial(link = "logit"), data=hotel_bookings)
anova_table <- Anova(new_model, test = "LR")
print(anova_table)
# Get the AIC
AIC(new_model)
#ASSESS
# Other diagnostic plots for the model
plot(new_model, which = c(1, 2))  # Residuals vs Fitted and Normal Q-Q plot
#ASSESS
# Other diagnostic plots for the model
plot(new_model, which = c(1, 2))  # Residuals vs Fitted and Normal Q-Q plot
# Create an empirical logit plot for the entire model
plot_model_fit <- function(model, bins = 10) {
# Get predicted probabilities from the model
pred_probs <- predict(model, type = "response")
# Get actual responses (as 0/1)
actual <- as.numeric(model$model[[1]]) - 1
# Create bins based on predicted probabilities
breaks <- quantile(pred_probs, probs = seq(0, 1, length.out = bins + 1))
bin_groups <- cut(pred_probs, breaks = breaks, include.lowest = TRUE)
# Calculate empirical logit for each bin
bin_data <- data.frame(
bin = bin_groups,
pred = pred_probs,
actual = actual
)
# Calculate average predicted probability and observed proportion for each bin
bin_summary <- aggregate(cbind(pred, actual) ~ bin, data = bin_data,
FUN = function(x) c(mean(x)))
# Calculate empirical logit
bin_summary$logit_pred <- log(bin_summary$pred / (1 - bin_summary$pred))
bin_summary$logit_actual <- log((bin_summary$actual + 0.5/nrow(bin_summary)) /
(1 - bin_summary$actual + 0.5/nrow(bin_summary)))
# Count observations per bin
bin_counts <- table(bin_groups)
bin_summary$count <- as.vector(bin_counts)
# Create plot
plot(bin_summary$logit_pred, bin_summary$logit_actual,
xlab = "Predicted logit", ylab = "Observed logit",
main = "Empirical Logit Plot for Model Fit",
pch = 19, cex = sqrt(bin_summary$count / max(bin_summary$count)) * 2)
# Add reference line (y = x)
abline(0, 1, col = "red", lty = 2)
# Add regression line
abline(lm(logit_actual ~ logit_pred, data = bin_summary,
weights = count), col = "blue")
# Add legend
legend("topleft", legend = c("Perfect fit", "Actual fit"),
col = c("red", "blue"), lty = c(2, 1))
return(bin_summary)
}
# Create the empirical logit plot for the model
model_fit <- plot_model_fit(new_model, bins = 10)
# Print the bin summary data
print(model_fit)
#ASSESS
# Other diagnostic plots for the model
plot(new_model, which = c(1, 2))  # Residuals vs Fitted and Normal Q-Q plot
# Create an empirical logit plot for the entire model
plot_model_fit <- function(model, bins = 10) {
# Get predicted probabilities from the model
pred_probs <- predict(model, type = "response")
# Get actual responses (as 0/1)
actual <- as.numeric(model$model[[1]]) - 1
# Create bins based on predicted probabilities
breaks <- quantile(pred_probs, probs = seq(0, 1, length.out = bins + 1))
bin_groups <- cut(pred_probs, breaks = breaks, include.lowest = TRUE)
# Calculate empirical logit for each bin
bin_data <- data.frame(
bin = bin_groups,
pred = pred_probs,
actual = actual
)
# Calculate average predicted probability and observed proportion for each bin
bin_summary <- aggregate(cbind(pred, actual) ~ bin, data = bin_data,
FUN = function(x) c(mean(x)))
# Calculate empirical logit
bin_summary$logit_pred <- log(bin_summary$pred / (1 - bin_summary$pred))
bin_summary$logit_actual <- log((bin_summary$actual + 0.5/nrow(bin_summary)) /
(1 - bin_summary$actual + 0.5/nrow(bin_summary)))
# Count observations per bin
bin_counts <- table(bin_groups)
bin_summary$count <- as.vector(bin_counts)
# Create plot
plot(bin_summary$logit_pred, bin_summary$logit_actual,
xlab = "Predicted logit", ylab = "Observed logit",
main = "Empirical Logit Plot for Model Fit",
pch = 19, cex = sqrt(bin_summary$count / max(bin_summary$count)) * 2)
# Add reference line (y = x)
abline(0, 1, col = "red", lty = 2)
# Add regression line
abline(lm(logit_actual ~ logit_pred, data = bin_summary,
weights = count), col = "blue")
# Add legend
legend("topleft", legend = c("Perfect fit", "Actual fit"),
col = c("red", "blue"), lty = c(2, 1))
return(bin_summary)
}
# Create the empirical logit plot for the model
model_fit <- plot_model_fit(new_model, bins = 10)
# Print the bin summary data
#print(model_fit)
#Cross validation
# Cross-validation for logistic regression model
# Set seed for reproducibility
set.seed(123)
# Define number of folds
k <- 10
# Create folds
folds <- sample(1:k, nrow(hotel_bookings), replace = TRUE)
# Initialize performance metrics
cv_metrics <- data.frame(
fold = 1:k,
accuracy = numeric(k),
sensitivity = numeric(k),
specificity = numeric(k),
auc = numeric(k)
)
# Perform k-fold cross-validation
for (i in 1:k) {
# Split data into training and test sets
train_data <- hotel_bookings[folds != i, ]
test_data <- hotel_bookings[folds == i, ]
# Fit model on training data
cv_model <- glm(factor(Status) ~ LeadTime + factor(Market) + HasRequests +
factor(Month) + AvgPrice + StayLength,
family = binomial(link = "logit"),
data = train_data)
# Make predictions on test data
predictions <- predict(cv_model, newdata = test_data, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)
actual_class <- as.numeric(factor(test_data$Status)) - 1
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = actual_class)
# Calculate performance metrics
if (dim(conf_matrix)[1] == 2 && dim(conf_matrix)[2] == 2) {
# When both classes are present in the fold
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
specificity <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
# Calculate AUC
library(pROC)
roc_obj <- roc(actual_class, predictions)
auc_value <- auc(roc_obj)
} else {
# Handle edge case when a fold might be missing a class
accuracy <- sum(predicted_class == actual_class) / length(actual_class)
sensitivity <- NA
specificity <- NA
auc_value <- NA
}
# Store metrics for this fold
cv_metrics$accuracy[i] <- accuracy
cv_metrics$sensitivity[i] <- sensitivity
cv_metrics$specificity[i] <- specificity
cv_metrics$auc[i] <- auc_value
}
# Calculate average performance across folds
cv_summary <- colMeans(cv_metrics[, -1], na.rm = TRUE)
cv_sd <- apply(cv_metrics[, -1], 2, sd, na.rm = TRUE)
# Print results
print("Cross-validation results:")
print(cv_metrics)
print("Average performance:")
print(cv_summary)
print("Standard deviation of performance:")
print(cv_sd)
# Plot results
par(mfrow = c(2, 2))
boxplot(cv_metrics$accuracy, main = "Accuracy", ylim = c(0, 1))
boxplot(cv_metrics$sensitivity, main = "Sensitivity", ylim = c(0, 1))
boxplot(cv_metrics$specificity, main = "Specificity", ylim = c(0, 1))
boxplot(cv_metrics$auc, main = "AUC", ylim = c(0, 1))
par(mfrow = c(1, 1))
# quantitative EDA
# Compare means of each quantitative variable between canceled and not canceled groups
group_comparison <- hotel_bookings %>%
group_by(Status) %>%
summarize(
Mean_LeadTime = mean(LeadTime, na.rm = TRUE),
Mean_AvgPrice = mean(AvgPrice, na.rm = TRUE),
Mean_GroupSize = mean(GroupSize, na.rm = TRUE),
Mean_StayLength = mean(StayLength, na.rm = TRUE)
)
# Round numeric columns to 3 decimal places
group_comparison_formatted <- group_comparison %>%
mutate(across(where(is.numeric), ~round(., 3)))
# Create pretty kable table
kable(group_comparison_formatted, caption = "Quantitative Explanatory Variables") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE) %>%
column_spec(1, bold = TRUE) %>%
row_spec(0, bold = TRUE, background = "#E8E8E8")
hotel_long <- hotel_bookings %>%
select(Status, LeadTime, AvgPrice, GroupSize, StayLength) %>%
pivot_longer(cols = c(LeadTime, AvgPrice, GroupSize, StayLength),
names_to = "Variable",
values_to = "Value")
# faceted boxplots for quantitative variables
ggplot(hotel_long, aes(x = Status, y = Value, fill = Status)) +
geom_boxplot() +
facet_wrap(~ Variable, scales = "free_y") +
scale_fill_manual(values = c("Canceled" = "#FF6B6B", "Not_Canceled" = "#4ECDC4")) +
labs(title = "Comparison of quantitative variables by cancellation status") +
theme_minimal()
#Cross validation
# Set seed for reproducibility
set.seed(123)
# Define number of folds
k <- 10
# Create folds
folds <- sample(1:k, nrow(hotel_bookings), replace = TRUE)
# Initialize performance metrics
cv_metrics <- data.frame(
fold = 1:k,
accuracy = numeric(k),
sensitivity = numeric(k),
specificity = numeric(k),
auc = numeric(k)
)
# Perform k-fold cross-validation
for (i in 1:k) {
# Split data into training and test sets
train_data <- hotel_bookings[folds != i, ]
test_data <- hotel_bookings[folds == i, ]
# Fit model on training data
cv_model <- glm(factor(Status) ~ LeadTime + factor(Market) + HasRequests +
factor(Month) + AvgPrice + StayLength,
family = binomial(link = "logit"),
data = train_data)
# Make predictions on test data
predictions <- predict(cv_model, newdata = test_data, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)
actual_class <- as.numeric(factor(test_data$Status)) - 1
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = actual_class)
# Calculate performance metrics
if (dim(conf_matrix)[1] == 2 && dim(conf_matrix)[2] == 2) {
# When both classes are present in the fold
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
specificity <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
# Calculate AUC
library(pROC)
roc_obj <- roc(actual_class, predictions)
auc_value <- auc(roc_obj)
} else {
# Handle edge case when a fold might be missing a class
accuracy <- sum(predicted_class == actual_class) / length(actual_class)
sensitivity <- NA
specificity <- NA
auc_value <- NA
}
# Store metrics for this fold
cv_metrics$accuracy[i] <- accuracy
cv_metrics$sensitivity[i] <- sensitivity
cv_metrics$specificity[i] <- specificity
cv_metrics$auc[i] <- auc_value
}
# Calculate average performance across folds
cv_summary <- colMeans(cv_metrics[, -1], na.rm = TRUE)
cv_sd <- apply(cv_metrics[, -1], 2, sd, na.rm = TRUE)
# Print results
print("Cross-validation results:")
print(cv_metrics)
print("Average performance:")
print(cv_summary)
print("Standard deviation of performance:")
print(cv_sd)
#Cross validation
# Set seed for reproducibility
set.seed(123)
# Define number of folds
k <- 10
# Create folds
folds <- sample(1:k, nrow(hotel_bookings), replace = TRUE)
# Initialize performance metrics
cv_metrics <- data.frame(
fold = 1:k,
accuracy = numeric(k),
sensitivity = numeric(k),
specificity = numeric(k),
auc = numeric(k)
)
# Perform k-fold cross-validation
for (i in 1:k) {
# Split data into training and test sets
train_data <- hotel_bookings[folds != i, ]
test_data <- hotel_bookings[folds == i, ]
# Fit model on training data
cv_model <- glm(factor(Status) ~ LeadTime + factor(Market) + HasRequests +
factor(Month) + AvgPrice + StayLength,
family = binomial(link = "logit"),
data = train_data)
# Make predictions on test data
predictions <- predict(cv_model, newdata = test_data, type = "response")
predicted_class <- ifelse(predictions > 0.5, 1, 0)
actual_class <- as.numeric(factor(test_data$Status)) - 1
# Create confusion matrix
conf_matrix <- table(Predicted = predicted_class, Actual = actual_class)
# Calculate performance metrics
if (dim(conf_matrix)[1] == 2 && dim(conf_matrix)[2] == 2) {
# When both classes are present in the fold
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
sensitivity <- conf_matrix[2, 2] / sum(conf_matrix[, 2])
specificity <- conf_matrix[1, 1] / sum(conf_matrix[, 1])
# Calculate AUC
library(pROC)
roc_obj <- roc(actual_class, predictions)
auc_value <- auc(roc_obj)
} else {
# Handle edge case when a fold might be missing a class
accuracy <- sum(predicted_class == actual_class) / length(actual_class)
sensitivity <- NA
specificity <- NA
auc_value <- NA
}
# Store metrics for this fold
cv_metrics$accuracy[i] <- accuracy
cv_metrics$sensitivity[i] <- sensitivity
cv_metrics$specificity[i] <- specificity
cv_metrics$auc[i] <- auc_value
}
# Calculate average performance across folds
cv_summary <- colMeans(cv_metrics[, -1], na.rm = TRUE)
cv_sd <- apply(cv_metrics[, -1], 2, sd, na.rm = TRUE)
# Print results
print("Cross-validation results:")
print(cv_metrics)
print("Average performance:")
print(cv_summary)
print("Standard deviation of performance:")
#data summary of folds
#print(cv_sd)
